{
 "archived": false,
 "branch": "main",
 "conda-forge.yml": {
  "azure": {
   "free_disk_space": true,
   "settings_linux": {
    "swapfile_size": "10GiB"
   },
   "timeout_minutes": 360
  },
  "conda_build": {
   "error_overlinking": true
  },
  "conda_forge_output_validation": true,
  "github": {
   "branch_name": "main",
   "tooling_branch_name": "main"
  }
 },
 "feedstock_name": "flash-attn",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "home": "https://github.com/Dao-AILab/flash-attention",
   "license": "BSD-3-Clause",
   "license_file": [
    "LICENSE",
    "LICENSE_CUTLASS.txt"
   ],
   "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention"
  },
  "build": {
   "ignore_run_exports_from": [
    "libcublas-dev",
    "libcusolver-dev",
    "libcusparse-dev"
   ],
   "number": "0",
   "rpaths": [
    "lib/",
    "SP_DIR/torch/lib/"
   ],
   "script": "PYTHON -m pip install . -vvv --no-deps --no-build-isolation",
   "script_env": [
    "MAX_JOBS=$CPU_COUNT",
    "TORCH_CUDA_ARCH_LIST=8.0+PTX"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "carterbox",
    "weiji14"
   ]
  },
  "package": {
   "name": "flash-attn",
   "version": "2.6.1"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "c_stdlib_stub",
    "ninja"
   ],
   "host": [
    "cuda-version 11.8",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "pytorch =*=cuda*",
    "setuptools",
    "cuda-version 12.0",
    "cuda-cudart-dev",
    "libcublas-dev",
    "libcusolver-dev",
    "libcusparse-dev"
   ],
   "run": [
    "einops",
    "python",
    "pytorch =*=cuda*"
   ]
  },
  "source": [
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   }
  ],
  "test": {
   "commands": [
    "pip check"
   ],
   "imports": [
    "flash_attn"
   ],
   "requires": [
    "pip"
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "c_stdlib_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-cudart-dev",
    "cuda-version",
    "libcublas-dev",
    "libcusolver-dev",
    "libcusparse-dev",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "setuptools"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "einops",
    "python",
    "pytorch"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "meta_yaml": {
  "about": {
   "home": "https://github.com/Dao-AILab/flash-attention",
   "license": "BSD-3-Clause",
   "license_file": [
    "LICENSE",
    "LICENSE_CUTLASS.txt"
   ],
   "summary": "Flash Attention: Fast and Memory-Efficient Exact Attention"
  },
  "build": {
   "ignore_run_exports_from": [
    "libcublas-dev",
    "libcusolver-dev",
    "libcusparse-dev"
   ],
   "number": "0",
   "rpaths": [
    "lib/",
    "SP_DIR/torch/lib/"
   ],
   "script": "PYTHON -m pip install . -vvv --no-deps --no-build-isolation",
   "script_env": [
    "MAX_JOBS=$CPU_COUNT",
    "TORCH_CUDA_ARCH_LIST=8.0+PTX"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "carterbox",
    "weiji14"
   ]
  },
  "package": {
   "name": "flash-attn",
   "version": "2.6.1"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "c_stdlib_stub",
    "ninja"
   ],
   "host": [
    "cuda-version 11.8",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "pytorch =*=cuda*",
    "setuptools",
    "cuda-version 12.0",
    "cuda-cudart-dev",
    "libcublas-dev",
    "libcusolver-dev",
    "libcusparse-dev"
   ],
   "run": [
    "einops",
    "python",
    "pytorch =*=cuda*"
   ]
  },
  "source": [
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   },
   {
    "sha256": "c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3",
    "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz"
   },
   {
    "path": "pyproject.toml"
   },
   {
    "path": "setup.py"
   }
  ],
  "test": {
   "commands": [
    "pip check"
   ],
   "imports": [
    "flash_attn"
   ],
   "requires": [
    "pip"
   ]
  }
 },
 "name": "flash-attn",
 "outputs_names": {
  "__set__": true,
  "elements": [
   "flash-attn"
  ]
 },
 "parsing_error": false,
 "platforms": [
  "linux_64"
 ],
 "pr_info": {
  "__lazy_json__": "pr_info/flash-attn.json"
 },
 "raw_meta_yaml": "{% set name = \"flash-attn\" %}\n{% set version = \"2.6.1\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  - url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/flash_attn-{{ version }}.tar.gz\n    sha256: c18d22d27031a761e68ffeb770be17b2b865b04fb9b401aa35372957965d01a3\n  # Overwrite with a simpler build script that doesn't try to revend pre-compiled binaries\n  - path: pyproject.toml\n  - path: setup.py\n\nbuild:\n  number: 0\n  script: {{ PYTHON }} -m pip install . -vvv --no-deps --no-build-isolation\n  script_env:\n    - MAX_JOBS=$CPU_COUNT\n    # Not compiling for 8.0;8.6;8.9;9.0+PTX to keep builds under 6 hours\n    - TORCH_CUDA_ARCH_LIST=8.0+PTX\n  skip: true  # [cuda_compiler_version in (undefined, \"None\")]\n  skip: true  # [not linux]\n  ignore_run_exports_from:\n    - libcublas-dev    # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libcusolver-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libcusparse-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n  rpaths:\n    - lib/\n    # PyTorch libs are in site-packages instead of with other shared objects\n    - {{ SP_DIR }}/torch/lib/\n\nrequirements:\n  build:\n    - {{ compiler('c') }}\n    - {{ compiler('cxx') }}\n    - {{ compiler('cuda') }}\n    - {{ stdlib('c') }}\n    - ninja\n  host:\n    - cuda-version {{ cuda_compiler_version }}  # same cuda for host and build\n    - cuda-cudart-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libcublas-dev    # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libcusolver-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libcusparse-dev  # [(cuda_compiler_version or \"\").startswith(\"12\")]\n    - libtorch         # required until pytorch run_exports libtorch\n    - pip\n    - python\n    - pytorch\n    - pytorch =*=cuda*\n    - setuptools\n  run:\n    - einops\n    - python\n    - pytorch =*=cuda*\n\ntest:\n  imports:\n    - flash_attn\n  commands:\n    - pip check\n  requires:\n    - pip\n\nabout:\n  home: https://github.com/Dao-AILab/flash-attention\n  summary: 'Flash Attention: Fast and Memory-Efficient Exact Attention'\n  license: BSD-3-Clause\n  license_file:\n    - LICENSE\n    - LICENSE_CUTLASS.txt\n\nextra:\n  recipe-maintainers:\n    - carterbox\n    - weiji14\n",
 "req": {
  "__set__": true,
  "elements": [
   "c_compiler_stub",
   "c_stdlib_stub",
   "cuda-cudart-dev",
   "cuda-version",
   "cuda_compiler_stub",
   "cxx_compiler_stub",
   "einops",
   "libcublas-dev",
   "libcusolver-dev",
   "libcusparse-dev",
   "libtorch",
   "ninja",
   "pip",
   "python",
   "pytorch",
   "setuptools"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "c_stdlib_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "c_stdlib_stub",
    "cuda-cudart-dev",
    "cuda-version",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "libcublas-dev",
    "libcusolver-dev",
    "libcusparse-dev",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "setuptools"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "c_stdlib_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "einops",
    "python",
    "pytorch"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "c_stdlib_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "ninja"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cuda-cudart-dev",
    "cuda-version 11.8",
    "cuda-version 12.0",
    "libcublas-dev",
    "libcusolver-dev",
    "libcusparse-dev",
    "libtorch",
    "pip",
    "python",
    "pytorch",
    "pytorch =*=cuda*",
    "setuptools"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "einops",
    "python",
    "pytorch =*=cuda*"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip"
   ]
  }
 },
 "url": "https://pypi.io/packages/source/f/flash-attn/flash_attn-2.6.1.tar.gz",
 "version": "2.6.1",
 "version_pr_info": {
  "__lazy_json__": "version_pr_info/flash-attn.json"
 }
}